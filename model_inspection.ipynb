{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T05:52:03.717369Z",
     "start_time": "2024-06-29T05:52:03.712925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "names = [\n",
    "    \"/h.0/Add\"\n",
    "]\n",
    "names = [n + \"_output_0\" for n in names]\n",
    "# names += [\"/h.0/attn/Split_output_1\"]"
   ],
   "id": "631e43d882ce9d57",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T05:52:03.741600Z",
     "start_time": "2024-06-29T05:52:03.719402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"models/tinyphysics.onnx\")\n",
    "\n",
    "value_info_protos = []\n",
    "shape_info = onnx.shape_inference.infer_shapes(model)\n",
    "for node in shape_info.graph.value_info:\n",
    "    if node.name in names:\n",
    "        value_info_protos.append(node)\n",
    "model.graph.output.extend(value_info_protos)\n",
    "onnx.checker.check_model(model, full_check=True)\n",
    "onnx.save(model, \"models/tmp_tinyphysics_output.onnx\")"
   ],
   "id": "4fe188ac90072fe2",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T05:52:03.798661Z",
     "start_time": "2024-06-29T05:52:03.742330Z"
    }
   },
   "cell_type": "code",
   "source": "onnx.utils.extract_model(\"models/tmp_tinyphysics_output.onnx\", \"models/tmp_tinyphysics_extracted.onnx\", [\"states\", \"tokens\"], names)",
   "id": "52fd97f25c9fce65",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T05:52:03.818431Z",
     "start_time": "2024-06-29T05:52:03.799277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# insert node after \"/wt2_embedding/Gather\" to reshape\n",
    "node = None\n",
    "i = -1\n",
    "for i, n in enumerate(model.graph.node):\n",
    "    if n.name == \"/wt2_embedding/Gather\":\n",
    "        node = n\n",
    "        break\n",
    "assert node is not None\n",
    "\n",
    "reshaped = [1, 2, 0]\n",
    "model.graph.initializer.extend([\n",
    "    onnx.helper.make_tensor(\n",
    "        name=\"jl_reshape_shape\",\n",
    "        data_type=onnx.TensorProto.INT64,\n",
    "        dims=[len(reshaped)],\n",
    "        vals=reshaped,\n",
    "    )\n",
    "])\n",
    "reshape_node = onnx.helper.make_node(\n",
    "    \"Reshape\",\n",
    "    inputs=[node.output[0], \"jl_reshape_shape\"],\n",
    "    outputs=[\"/wt2_embedding/Gather_output_0_reshaped\"],\n",
    "    name=\"jl_reshape\",\n",
    ")\n",
    "\n",
    "# reassign input of next node\n",
    "for j in range(i + 1, len(model.graph.node)):\n",
    "    for k in range(len(model.graph.node[j].input)):\n",
    "        if model.graph.node[j].input[k] == node.output[0]:\n",
    "            model.graph.node[j].input[k] = \"/wt2_embedding/Gather_output_0_reshaped\"\n",
    "\n",
    "model.graph.node.insert(i + 1, reshape_node)\n",
    "\n",
    "onnx.save(model, \"models/tmp_tinyphysics_reshaped.onnx\")\n",
    "onnx.checker.check_model(model, full_check=True)"
   ],
   "id": "fc14c6e322ab7a19",
   "outputs": [
    {
     "ename": "InferenceError",
     "evalue": "[ShapeInferenceError] Inference error(s): (op_type:Concat, node name: /Concat_1): [ShapeInferenceError] Can't merge shape info. Both inferred and declared dimension have values but they differ. Inferred=2 Declared=20 Dimension=1\n(op_type:Add, node name: /Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:ReduceMean, node name: /h.0/attn/layer_norm/ReduceMean): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Sub, node name: /h.0/attn/layer_norm/Sub): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Pow, node name: /h.0/attn/layer_norm/Pow): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:ReduceMean, node name: /h.0/attn/layer_norm/ReduceMean_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/attn/layer_norm/Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Sqrt, node name: /h.0/attn/layer_norm/Sqrt): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Div, node name: /h.0/attn/layer_norm/Div): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Mul, node name: /h.0/attn/layer_norm/Mul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/attn/layer_norm/Add_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:MatMul, node name: /h.0/attn/c_attn/MatMul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Split, node name: /h.0/attn/Split): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape_2): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose_2): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:MatMul, node name: /h.0/attn/MatMul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Mul, node name: /h.0/attn/Mul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInferenceError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 35\u001B[0m\n\u001B[1;32m     32\u001B[0m model\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mnode\u001B[38;5;241m.\u001B[39minsert(i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, reshape_node)\n\u001B[1;32m     34\u001B[0m onnx\u001B[38;5;241m.\u001B[39msave(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/tmp_tinyphysics_reshaped.onnx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 35\u001B[0m \u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchecker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/controls_challenge/venv/lib/python3.11/site-packages/onnx/checker.py:179\u001B[0m, in \u001B[0;36mcheck_model\u001B[0;34m(model, full_check, skip_opset_compatibility_check, check_custom_domain)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mgetsizeof(protobuf_string) \u001B[38;5;241m>\u001B[39m MAXIMUM_PROTOBUF:\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis protobuf of onnx model is too large (>2GB). Call check_model with model path instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    178\u001B[0m     )\n\u001B[0;32m--> 179\u001B[0m \u001B[43mC\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprotobuf_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfull_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_opset_compatibility_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_custom_domain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mInferenceError\u001B[0m: [ShapeInferenceError] Inference error(s): (op_type:Concat, node name: /Concat_1): [ShapeInferenceError] Can't merge shape info. Both inferred and declared dimension have values but they differ. Inferred=2 Declared=20 Dimension=1\n(op_type:Add, node name: /Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:ReduceMean, node name: /h.0/attn/layer_norm/ReduceMean): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Sub, node name: /h.0/attn/layer_norm/Sub): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Pow, node name: /h.0/attn/layer_norm/Pow): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:ReduceMean, node name: /h.0/attn/layer_norm/ReduceMean_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/attn/layer_norm/Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Sqrt, node name: /h.0/attn/layer_norm/Sqrt): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Div, node name: /h.0/attn/layer_norm/Div): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Mul, node name: /h.0/attn/layer_norm/Mul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/attn/layer_norm/Add_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:MatMul, node name: /h.0/attn/c_attn/MatMul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Split, node name: /h.0/attn/Split): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Reshape, node name: /h.0/attn/Reshape_2): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose_1): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Transpose, node name: /h.0/attn/Transpose_2): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:MatMul, node name: /h.0/attn/MatMul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Mul, node name: /h.0/attn/Mul): [TypeInferenceError] Input 0 expected to have type but instead is null\n(op_type:Add, node name: /h.0/Add): [TypeInferenceError] Input 0 expected to have type but instead is null\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4905c23515117ffb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
